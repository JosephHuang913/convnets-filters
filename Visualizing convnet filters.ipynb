{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing what convnets learn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "It is often said that deep learning models are \"black boxes\", learning representations that are difficult to extract and present in a human-readable form. While this is partially true for certain types of deep learning models, it is definitely not true for convnets. The representations learned by convnets are highly amenable to visualization, in large part because they are _representations of visual concepts_.\n",
    "\n",
    "Visualizing convnets filters is useful to understand precisely what visual pattern or concept each filter in a convnet is receptive to.\n",
    "\n",
    "We will use the VGG16 model to Visualiz convnets filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing convnet filters\n",
    "\n",
    "\n",
    "Visualizing convnets filters is to display the visual pattern that each filter is meant to respond \n",
    "to. This can be done with __gradient ascent in input space__: applying __gradient descent__ to the value of the input image of a convnet so \n",
    "as to maximize the response of a specific filter, starting from a blank input image. The resulting input image would be one that the chosen \n",
    "filter is maximally responsive to.\n",
    "\n",
    "The process is simple: we will build a loss function that maximizes the value of a given filter in a given convolution layer, then we \n",
    "will use stochastic gradient descent to adjust the values of the input image so as to maximize this activation value. For instance, here's \n",
    "a loss for the activation of filter 0 in the layer \"block3_conv1\" of the VGG16 network, pre-trained on ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras import backend as K\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = VGG16(weights='imagenet',\n",
    "              include_top=False)\n",
    "\n",
    "layer_name = 'block3_conv1'\n",
    "filter_index = 0\n",
    "\n",
    "layer_output = model.get_layer(layer_name).output\n",
    "loss = K.mean(layer_output[:, :, :, filter_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement gradient descent, we will need the gradient of this loss with respect to the model's input. To do this, we will use the \n",
    "`gradients` function packaged with the `backend` module of Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The call to `gradients` returns a list of tensors (of size 1 in this case)\n",
    "# hence we only keep the first element -- which is a tensor.\n",
    "grads = K.gradients(loss, model.input)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non-obvious trick to use for the gradient descent process to go smoothly is to normalize the gradient tensor, by dividing it by its L2 \n",
    "norm (the square root of the average of the square of the values in the tensor). This ensures that the magnitude of the updates done to the \n",
    "input image is always within a same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add 1e-5 before dividing so as to avoid accidentally dividing by 0.\n",
    "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a way to compute the value of the loss tensor and the gradient tensor, given an input image. We can define a Keras backend \n",
    "function to do this: `iterate` is a function that takes a Numpy tensor (as a list of tensors of size 1) and returns a list of two Numpy \n",
    "tensors: the loss value and the gradient value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate = K.function([model.input], [loss, grads])\n",
    "\n",
    "# Let's test it:\n",
    "import numpy as np\n",
    "loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We define a Python loop to do stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start from a gray image with some noise\n",
    "input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\n",
    "\n",
    "# Run gradient ascent for 40 steps\n",
    "step = 1.  # this is the magnitude of each gradient update\n",
    "for i in range(40):\n",
    "    # Compute the loss value and gradient value\n",
    "    loss_value, grads_value = iterate([input_img_data])\n",
    "    # Here we adjust the input image in the direction that maximizes the loss\n",
    "    input_img_data += grads_value * step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting image tensor will be a floating point tensor of shape `(1, 150, 150, 3)`, with values that may not be integer within `[0, 255]`. Hence we would need to post-process this tensor to turn it into a displayable image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put all the pieces together into a Python function that takes as input a layer name and a filter index, and that returns a valid image tensor representing the pattern that maximizes the activation the specified filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pattern(layer_name, filter_index, size=150):\n",
    "    # Build a loss function that maximizes the activation of the nth filter of the layer considered.\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # Compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "\n",
    "    # Normalization trick: we normalize the gradient\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "    # This function returns the loss and grads given the input picture\n",
    "    iterate = K.function([model.input], [loss, grads])\n",
    "    \n",
    "    # We start from a gray image with some noise\n",
    "    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n",
    "\n",
    "    # Run gradient ascent for 50 steps\n",
    "    step = 1.\n",
    "    for i in range(50):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "        \n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generate_pattern('block3_conv1', 0, size=256))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that filter 0 in layer `block3_conv1` is responsive to a polka dot pattern.\n",
    "\n",
    "Now the fun part: we can start visualising every single filter in every layer. For simplicity, we will only look at the first 64 filters in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = ['block1_conv1', 'block1_conv2', \n",
    "               'block2_conv1', 'block2_conv2', \n",
    "               'block3_conv1', 'block3_conv2', 'block3_conv3', \n",
    "               'block4_conv1', 'block4_conv2', 'block4_conv3',\n",
    "               'block5_conv1', 'block5_conv2', 'block5_conv3']\n",
    "size = 128\n",
    "f_idx = 64\n",
    "\n",
    "\n",
    "for layer_name in layer_names:\n",
    "    plt.figure(num = 'show' ,figsize=(20,20))\n",
    "    for i in range(f_idx):  # iterate over the rows of our results grid\n",
    "        filter_img = generate_pattern(layer_name, i, size=size)\n",
    "        \n",
    "        # Display the results grid\n",
    "        plt.subplot(8, f_idx/8, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(filter_img)\n",
    "    #plt.savefig(layer_name+'.png', format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These filter visualizations tell us a lot about how convnet layers see the world: each layer in a convnet simply learns a collection of \n",
    "filters such that their inputs can be expressed as a combination of the filters. This is similar to how the Fourier transform decomposes \n",
    "signals onto a bank of cosine functions. The filters in these convnet filter banks get increasingly complex and refined as we go higher-up \n",
    "in the model:\n",
    "\n",
    "* The filters from the first layer in the model (`block1_conv1`) encode simple directional edges and colors (or colored edges in some \n",
    "cases).\n",
    "* The filters from `block2_conv1` encode simple textures made from combinations of edges and colors.\n",
    "* The filters in higher-up layers start resembling textures found in natural images: feathers, eyes, leaves, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, please refer to [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
